{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "w06ybld72igj5zcmkuqmjs",
    "execution_id": "7e48abeb-98eb-4135-90d9-fe96c4b6648b"
   },
   "source": [
    "<a id='section-id0'></a>\n",
    "# Working with data in Yandex DataSphere\n",
    "\n",
    "1. [Working with primary data sources](#section-id1)\n",
    "1. [Datasets](#section-id2)\n",
    "1. [Sharing output](#section-id3) \n",
    "1. [More about Yandex DataSphere](#section-id4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:33:45.938236Z",
     "iopub.status.busy": "2025-07-03T16:33:45.937133Z",
     "iopub.status.idle": "2025-07-03T16:34:25.481657Z",
     "shell.execute_reply": "2025-07-03T16:34:25.480582Z",
     "shell.execute_reply.started": "2025-07-03T16:33:45.938189Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install boto3 if needed\n",
    "# set os environment variables aws_access_key_id and aws_secret_access_key\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name: str, local_path: str, file_name: str) -> None:\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_path = Path.joinpath(local_path, file_name)\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    s3_client.download_file(\n",
    "        bucket_name,\n",
    "        file_name,\n",
    "        str(file_path)\n",
    "    )\n",
    "\n",
    "\n",
    "S3_CREDS = {\n",
    "    \"aws_access_key_id\": 'YCAJEAeZbsb8c7fvN8GNL2vXz',\n",
    "    \"aws_secret_access_key\": 'YCNNhq867MpjKO1w5vZhuCvHlpO64mCyI4tkJeOq'\n",
    "}\n",
    "\n",
    "bucket = \"bucket-datalens-test-abacaba\"\n",
    "file_name = 'part-0.parquet'\n",
    "\n",
    "client = boto3.client(\n",
    "    service_name='s3',\n",
    "    endpoint_url='https://storage.yandexcloud.net',\n",
    "    **S3_CREDS)\n",
    "\n",
    "download_files(\n",
    "    client,\n",
    "    bucket,\n",
    "    \"from-s3-folder\",\n",
    "    file_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:36:40.048709Z",
     "iopub.status.busy": "2025-07-03T16:36:40.047252Z",
     "iopub.status.idle": "2025-07-03T16:37:14.027874Z",
     "shell.execute_reply": "2025-07-03T16:37:14.026634Z",
     "shell.execute_reply.started": "2025-07-03T16:36:40.048662Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyarrow==20.0.*\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyarrow-20.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow==20.0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:38:22.257907Z",
     "iopub.status.busy": "2025-07-03T16:38:22.256355Z",
     "iopub.status.idle": "2025-07-03T16:38:23.303213Z",
     "shell.execute_reply": "2025-07-03T16:38:23.302231Z",
     "shell.execute_reply.started": "2025-07-03T16:38:22.257844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "parquet_file = pq.ParquetFile('./from-s3-folder/part-0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:48:10.467745Z",
     "iopub.status.busy": "2025-07-03T16:48:10.466730Z",
     "iopub.status.idle": "2025-07-03T16:48:10.485673Z",
     "shell.execute_reply": "2025-07-03T16:48:10.484780Z",
     "shell.execute_reply.started": "2025-07-03T16:48:10.467696Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7f0e650358f0>\n",
       "  created_by: parquet-cpp-arrow version 16.1.0\n",
       "  num_columns: 213\n",
       "  num_rows: 3191103\n",
       "  num_row_groups: 1729\n",
       "  format_version: 2.6\n",
       "  serialized_size: 38489415"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T17:01:55.629858Z",
     "iopub.status.busy": "2025-07-03T17:01:55.628749Z",
     "iopub.status.idle": "2025-07-03T17:01:55.644840Z",
     "shell.execute_reply": "2025-07-03T17:01:55.642909Z",
     "shell.execute_reply.started": "2025-07-03T17:01:55.629803Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ParquetSchema object at 0x7f0e650486c0>\n",
       "required group field_id=-1 schema {\n",
       "  optional binary field_id=-1 inn (String);\n",
       "  optional binary field_id=-1 ogrn (String);\n",
       "  optional binary field_id=-1 region (String);\n",
       "  optional binary field_id=-1 region_taxcode (String);\n",
       "  optional int32 field_id=-1 creation_date (Date);\n",
       "  optional int32 field_id=-1 dissolution_date (Date);\n",
       "  optional double field_id=-1 age;\n",
       "  optional double field_id=-1 eligible;\n",
       "  optional binary field_id=-1 exemption_criteria (String);\n",
       "  optional double field_id=-1 financial;\n",
       "  optional double field_id=-1 filed;\n",
       "  optional double field_id=-1 imputed;\n",
       "  optional double field_id=-1 simplified;\n",
       "  optional double field_id=-1 articulated;\n",
       "  optional double field_id=-1 totals_adjustment;\n",
       "  optional double field_id=-1 outlier;\n",
       "  optional binary field_id=-1 okved (String);\n",
       "  optional binary field_id=-1 okved_section (String);\n",
       "  optional binary field_id=-1 okpo (String);\n",
       "  optional binary field_id=-1 okopf (String);\n",
       "  optional binary field_id=-1 okogu (String);\n",
       "  optional binary field_id=-1 okfc (String);\n",
       "  optional binary field_id=-1 oktmo (String);\n",
       "  optional binary field_id=-1 lon (String);\n",
       "  optional binary field_id=-1 lat (String);\n",
       "  optional binary field_id=-1 geocoding_quality (String);\n",
       "  optional double field_id=-1 line_1100;\n",
       "  optional double field_id=-1 line_1110;\n",
       "  optional double field_id=-1 line_1120;\n",
       "  optional double field_id=-1 line_1130;\n",
       "  optional double field_id=-1 line_1140;\n",
       "  optional double field_id=-1 line_1150;\n",
       "  optional double field_id=-1 line_1160;\n",
       "  optional double field_id=-1 line_1170;\n",
       "  optional double field_id=-1 line_1180;\n",
       "  optional double field_id=-1 line_1190;\n",
       "  optional double field_id=-1 line_1200;\n",
       "  optional double field_id=-1 line_1210;\n",
       "  optional double field_id=-1 line_1220;\n",
       "  optional double field_id=-1 line_1230;\n",
       "  optional double field_id=-1 line_1240;\n",
       "  optional double field_id=-1 line_1250;\n",
       "  optional double field_id=-1 line_1260;\n",
       "  optional double field_id=-1 line_1300;\n",
       "  optional double field_id=-1 line_1310;\n",
       "  optional double field_id=-1 line_1320;\n",
       "  optional double field_id=-1 line_1340;\n",
       "  optional double field_id=-1 line_1350;\n",
       "  optional double field_id=-1 line_1360;\n",
       "  optional double field_id=-1 line_1370;\n",
       "  optional double field_id=-1 line_1400;\n",
       "  optional double field_id=-1 line_1410;\n",
       "  optional double field_id=-1 line_1420;\n",
       "  optional double field_id=-1 line_1430;\n",
       "  optional double field_id=-1 line_1450;\n",
       "  optional double field_id=-1 line_1500;\n",
       "  optional double field_id=-1 line_1510;\n",
       "  optional double field_id=-1 line_1520;\n",
       "  optional double field_id=-1 line_1530;\n",
       "  optional double field_id=-1 line_1540;\n",
       "  optional double field_id=-1 line_1550;\n",
       "  optional double field_id=-1 line_1600;\n",
       "  optional double field_id=-1 line_1700;\n",
       "  optional double field_id=-1 line_2100;\n",
       "  optional double field_id=-1 line_2110;\n",
       "  optional double field_id=-1 line_2120;\n",
       "  optional double field_id=-1 line_2200;\n",
       "  optional double field_id=-1 line_2210;\n",
       "  optional double field_id=-1 line_2220;\n",
       "  optional double field_id=-1 line_2300;\n",
       "  optional double field_id=-1 line_2310;\n",
       "  optional double field_id=-1 line_2320;\n",
       "  optional double field_id=-1 line_2330;\n",
       "  optional double field_id=-1 line_2340;\n",
       "  optional double field_id=-1 line_2350;\n",
       "  optional double field_id=-1 line_2400;\n",
       "  optional double field_id=-1 line_2410;\n",
       "  optional double field_id=-1 line_2411;\n",
       "  optional double field_id=-1 line_2412;\n",
       "  optional double field_id=-1 line_2421;\n",
       "  optional double field_id=-1 line_2430;\n",
       "  optional double field_id=-1 line_2450;\n",
       "  optional double field_id=-1 line_2460;\n",
       "  optional double field_id=-1 line_2500;\n",
       "  optional double field_id=-1 line_2510;\n",
       "  optional double field_id=-1 line_2520;\n",
       "  optional double field_id=-1 line_2530;\n",
       "  optional double field_id=-1 line_2900;\n",
       "  optional double field_id=-1 line_2910;\n",
       "  optional double field_id=-1 line_3100;\n",
       "  optional double field_id=-1 line_3200;\n",
       "  optional double field_id=-1 line_3210;\n",
       "  optional double field_id=-1 line_3211;\n",
       "  optional double field_id=-1 line_3212;\n",
       "  optional double field_id=-1 line_3213;\n",
       "  optional double field_id=-1 line_3214;\n",
       "  optional double field_id=-1 line_3215;\n",
       "  optional double field_id=-1 line_3216;\n",
       "  optional double field_id=-1 line_321x;\n",
       "  optional double field_id=-1 line_3220;\n",
       "  optional double field_id=-1 line_3221;\n",
       "  optional double field_id=-1 line_3222;\n",
       "  optional double field_id=-1 line_3223;\n",
       "  optional double field_id=-1 line_3224;\n",
       "  optional double field_id=-1 line_3225;\n",
       "  optional double field_id=-1 line_3226;\n",
       "  optional double field_id=-1 line_3227;\n",
       "  optional double field_id=-1 line_322x;\n",
       "  optional double field_id=-1 line_3230;\n",
       "  optional double field_id=-1 line_3240;\n",
       "  optional double field_id=-1 line_3300;\n",
       "  optional double field_id=-1 line_3310;\n",
       "  optional double field_id=-1 line_3311;\n",
       "  optional double field_id=-1 line_3312;\n",
       "  optional double field_id=-1 line_3313;\n",
       "  optional double field_id=-1 line_3314;\n",
       "  optional double field_id=-1 line_3315;\n",
       "  optional double field_id=-1 line_3316;\n",
       "  optional double field_id=-1 line_331x;\n",
       "  optional double field_id=-1 line_3320;\n",
       "  optional double field_id=-1 line_3321;\n",
       "  optional double field_id=-1 line_3322;\n",
       "  optional double field_id=-1 line_3323;\n",
       "  optional double field_id=-1 line_3324;\n",
       "  optional double field_id=-1 line_3325;\n",
       "  optional double field_id=-1 line_3326;\n",
       "  optional double field_id=-1 line_3327;\n",
       "  optional double field_id=-1 line_332x;\n",
       "  optional double field_id=-1 line_3330;\n",
       "  optional double field_id=-1 line_3340;\n",
       "  optional double field_id=-1 line_3400;\n",
       "  optional double field_id=-1 line_3401;\n",
       "  optional double field_id=-1 line_3402;\n",
       "  optional double field_id=-1 line_3410;\n",
       "  optional double field_id=-1 line_3411;\n",
       "  optional double field_id=-1 line_3412;\n",
       "  optional double field_id=-1 line_3420;\n",
       "  optional double field_id=-1 line_3421;\n",
       "  optional double field_id=-1 line_3422;\n",
       "  optional double field_id=-1 line_3500;\n",
       "  optional double field_id=-1 line_3501;\n",
       "  optional double field_id=-1 line_3502;\n",
       "  optional double field_id=-1 line_3600;\n",
       "  optional double field_id=-1 line_4100;\n",
       "  optional double field_id=-1 line_4110;\n",
       "  optional double field_id=-1 line_4111;\n",
       "  optional double field_id=-1 line_4112;\n",
       "  optional double field_id=-1 line_4113;\n",
       "  optional double field_id=-1 line_4119;\n",
       "  optional double field_id=-1 line_411x;\n",
       "  optional double field_id=-1 line_4120;\n",
       "  optional double field_id=-1 line_4121;\n",
       "  optional double field_id=-1 line_4122;\n",
       "  optional double field_id=-1 line_4123;\n",
       "  optional double field_id=-1 line_4124;\n",
       "  optional double field_id=-1 line_4129;\n",
       "  optional double field_id=-1 line_412x;\n",
       "  optional double field_id=-1 line_4200;\n",
       "  optional double field_id=-1 line_4210;\n",
       "  optional double field_id=-1 line_4211;\n",
       "  optional double field_id=-1 line_4212;\n",
       "  optional double field_id=-1 line_4213;\n",
       "  optional double field_id=-1 line_4214;\n",
       "  optional double field_id=-1 line_4219;\n",
       "  optional double field_id=-1 line_421x;\n",
       "  optional double field_id=-1 line_4220;\n",
       "  optional double field_id=-1 line_4221;\n",
       "  optional double field_id=-1 line_4222;\n",
       "  optional double field_id=-1 line_4223;\n",
       "  optional double field_id=-1 line_4224;\n",
       "  optional double field_id=-1 line_4229;\n",
       "  optional double field_id=-1 line_422x;\n",
       "  optional double field_id=-1 line_4300;\n",
       "  optional double field_id=-1 line_4310;\n",
       "  optional double field_id=-1 line_4311;\n",
       "  optional double field_id=-1 line_4312;\n",
       "  optional double field_id=-1 line_4313;\n",
       "  optional double field_id=-1 line_4314;\n",
       "  optional double field_id=-1 line_4319;\n",
       "  optional double field_id=-1 line_431x;\n",
       "  optional double field_id=-1 line_4320;\n",
       "  optional double field_id=-1 line_4321;\n",
       "  optional double field_id=-1 line_4322;\n",
       "  optional double field_id=-1 line_4323;\n",
       "  optional double field_id=-1 line_4329;\n",
       "  optional double field_id=-1 line_432x;\n",
       "  optional double field_id=-1 line_4400;\n",
       "  optional double field_id=-1 line_4450;\n",
       "  optional double field_id=-1 line_4490;\n",
       "  optional double field_id=-1 line_4500;\n",
       "  optional double field_id=-1 line_6100;\n",
       "  optional double field_id=-1 line_6200;\n",
       "  optional double field_id=-1 line_6210;\n",
       "  optional double field_id=-1 line_6215;\n",
       "  optional double field_id=-1 line_6220;\n",
       "  optional double field_id=-1 line_6230;\n",
       "  optional double field_id=-1 line_6240;\n",
       "  optional double field_id=-1 line_6250;\n",
       "  optional double field_id=-1 line_6300;\n",
       "  optional double field_id=-1 line_6310;\n",
       "  optional double field_id=-1 line_6311;\n",
       "  optional double field_id=-1 line_6312;\n",
       "  optional double field_id=-1 line_6313;\n",
       "  optional double field_id=-1 line_6320;\n",
       "  optional double field_id=-1 line_6321;\n",
       "  optional double field_id=-1 line_6322;\n",
       "  optional double field_id=-1 line_6323;\n",
       "  optional double field_id=-1 line_6324;\n",
       "  optional double field_id=-1 line_6325;\n",
       "  optional double field_id=-1 line_6326;\n",
       "  optional double field_id=-1 line_6330;\n",
       "  optional double field_id=-1 line_6350;\n",
       "  optional double field_id=-1 line_6400;\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T19:23:06.211945Z",
     "iopub.status.busy": "2025-07-03T19:23:06.210571Z",
     "iopub.status.idle": "2025-07-03T19:23:06.248612Z",
     "shell.execute_reply": "2025-07-03T19:23:06.247406Z",
     "shell.execute_reply.started": "2025-07-03T19:23:06.211894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T16:57:10.550798Z",
     "iopub.status.busy": "2025-07-03T16:57:10.549372Z",
     "iopub.status.idle": "2025-07-03T16:57:10.580506Z",
     "shell.execute_reply": "2025-07-03T16:57:10.579458Z",
     "shell.execute_reply.started": "2025-07-03T16:57:10.550730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = []\n",
    "for i in range(213):\n",
    "    col = parquet_file.schema.column(i)\n",
    "    t = col.physical_type.lower()\n",
    "    if t.startswith('byte'):\n",
    "        t = 'string'\n",
    "    s.append(dict(name=col.name, type=t, required=False))\n",
    "with open('out.txt', 'w') as f:\n",
    "    f.write(json.dumps(s))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "bj5g8m7b1samev84h2axd",
    "execution_id": "24a9522d-fa38-484b-8b5d-0b0e3f8a1f09",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='section-id1'></a>\n",
    "## 1. Working with data sources\n",
    "\n",
    "DataSphere includes the ability to handle all major data sources. Using standard Jupyter Notebook tools, you can copy existing notebooks and data from your local machine.\n",
    "You can work with Git, Yandex.Disk, Google Drive, S3, FTP, and Spark. To make it easier to work with all data sources, DataSphere has Snippets: examples of code describing the handling of data sources. You can also find connection examples in [our documentation](https://cloud.yandex.ru/docs/datasphere/operations/#data-source).\n",
    "\n",
    "![](https://storage.yandexcloud.net/onboarding-notebooks/screenshots/snippets.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "06zwgmtx0ei85pspit633zb"
   },
   "outputs": [],
   "source": [
    "# For example, you can get a cell like this if you select Snippets -> S3 -> Get file.py in the notebook top panel\n",
    "\n",
    "# %pip install boto3 if needed\n",
    "# set os environment variables aws_access_key_id and aws_secret_access_key\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name: str, local_path: str, file_name: str) -> None:\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_path = Path.joinpath(local_path, file_name)\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    s3_client.download_file(\n",
    "        bucket_name,\n",
    "        file_name,\n",
    "        str(file_path)\n",
    "    )\n",
    "\n",
    "\n",
    "S3_CREDS = {\n",
    "    \"aws_access_key_id\": os.environ['aws_access_key_id'],\n",
    "    \"aws_secret_access_key\": os.environ['aws_secret_access_key']\n",
    "}\n",
    "\n",
    "bucket = \"my_bucket\"\n",
    "file_name = 'path/to/file'\n",
    "\n",
    "client = boto3.client(\n",
    "    service_name='s3',\n",
    "    endpoint_url='https://storage.yandexcloud.net',\n",
    "    **S3_CREDS)\n",
    "\n",
    "download_files(\n",
    "    client,\n",
    "    bucket,\n",
    "    \"from-s3-folder\",\n",
    "    file_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "1aof0qfwefufx36ibguwte"
   },
   "source": [
    "In DataSphere, you can work with Git repositories, both local and remote. \n",
    "All steps are described in detail in [our documentation](https://cloud.yandex.ru/docs/datasphere/operations/projects/work-with-git)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vjunptlairw18irpraiab",
    "execution_id": "aee0e189-68ff-42ff-8500-35b63b1ca904"
   },
   "source": [
    "DataSphere works as a service and uses the Python shell for file system access. No terminal is available in DataSphere, but you can run Bash commands. To do this, specify the `#!:bash` prefix in the cell header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "wyeoqjirpdgzahlqvrm46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is yet another test\n"
     ]
    }
   ],
   "source": [
    "#!:bash \n",
    "\n",
    "# For example, you can perform an operation like this on a text string\n",
    "echo \"This is a test\" | sed 's/a test/yet another test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "099ddxqkm6xv3n6umvj7aqc",
    "execution_id": "233fa9a6-aae8-4af1-975b-90a17f2060d5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='section-id2'></a>\n",
    "\n",
    "## 2. Datasets\n",
    "\n",
    "Datasets are a convenient way of storing large sets of data that do not need to be modified during computations. Datasets can store up to 4 TB of data and provide faster access than project storage. \n",
    "\n",
    "You must populate a dataset immediately upon creation and initialization. After initialization, a dataset will become read-only.\n",
    "\n",
    "You can view all datasets of a project in the **Dataset** section in the project resources. You can create and initialize a dataset in a cell with the `#pragma dataset init` command. \n",
    "\n",
    "Here is the template you will get if you select **Snippets -> Datasets -> Create custom dataset.py** in the top menu. The minimum size of a new dataset is 1 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2dvexk2c9nbwxkkt57jo2l"
   },
   "outputs": [],
   "source": [
    "#pragma dataset init DATASET_NAME --size 1Gb\n",
    "\n",
    "# TODO: fill dataset here\n",
    "# Dataset will be created in /home/jupyter/mnt/datasets/DATASET_NAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "er7lczv99164bj8r7h6g6",
    "execution_id": "76016d2a-e688-4b11-a1d1-9f3705008bf0"
   },
   "source": [
    "Datasets can be populated from files using a link as well as file storage objects. For dataset creation examples, see [our documentation](https://cloud.yandex.ru/docs/datasphere/concepts/dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "89y7jyynbgcujvw857jx7",
    "execution_id": "aa379fd5-2075-4ee9-9fcb-fba99eb733aa",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='section-id3'></a>\n",
    "\n",
    "## 3. Sharing output\n",
    "\n",
    "#### Publishing a notebook\n",
    "\n",
    "You can export a notebook as a link to an HTML report. The link is active for a week.\n",
    "\n",
    "![](https://storage.yandexcloud.net/onboarding-notebooks/screenshots/export-notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "x148oam8qevcqp9hgeikj",
    "execution_id": "bb0fbd49-393d-4a91-9aac-350e3a1619f9"
   },
   "source": [
    "#### Exporting a notebook to external projects\n",
    "\n",
    "To export a notebook to external projects, you can use the procedure for working with Git as described above. \n",
    "You can also export a project as a ZIP archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qk7ai7w9cyfhamghgfewh",
    "execution_id": "38bb8831-3715-42d1-80fc-f011d79480d6",
    "tags": []
   },
   "source": [
    "<a id='section-id4'></a>\n",
    "\n",
    "## 4. More about Yandex DataSphere\n",
    "We have a detailed [documentation](https://cloud.yandex.ru/docs/datasphere/) available.\n",
    "\n",
    "**Videos and demos**\n",
    "\n",
    "- Presentation of a new DataSphere version with an updated interface: [Yandex DataSphere: New UI and collaborative ML development capabilities\n",
    "](https://www.youtube.com/watch?v=xzEW5g7WVd4&themeRefresh=1).\n",
    "\n",
    "- Enabling collaboration through a community and projects: [DataSphere features for distributed ML teams\n",
    "](https://www.youtube.com/watch?v=xM0qdz5wJdE) \n",
    "\n",
    "**Projects implemented in Yandex DataSphere**\n",
    "\n",
    "- Environmental monitoring of the Lake Baikal to forecast its state and impact of climate change on its ecosystem, as well as measure fish populations: [About project](https://cloud.yandex.ru/special/baikal/), [Habr](https://habr.com/ru/companies/yandex/articles/689592/), [GitHub](https://github.com/baikal-zooplankton)\n",
    "\n",
    "- Forecasting the El Niño natural anomaly in the Pacific Ocean: [About project](https://cloud.yandex.ru/blog/posts/2023/04/el-nino)\n",
    "\n",
    "Join the <a href=\"https://t.me/yandex_datasphere\">DataSphere community chat on Telegram</a>! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T19:38:23.621664Z",
     "iopub.status.busy": "2025-07-03T19:38:23.620288Z",
     "iopub.status.idle": "2025-07-03T19:38:29.010572Z",
     "shell.execute_reply": "2025-07-03T19:38:29.009345Z",
     "shell.execute_reply.started": "2025-07-03T19:38:23.621611Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: polar in /home/jupyter/.local/lib/python3.10/site-packages (0.0.127)\n",
      "Requirement already satisfied: seaborn in /home/jupyter/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in /home/jupyter/.local/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in /home/jupyter/.local/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: scipy in /home/jupyter/.local/lib/python3.10/site-packages (1.15.3)\n",
      "Requirement already satisfied: matplotlib in /home/jupyter/.local/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: nltk in /home/jupyter/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from polar) (0.13.5)\n",
      "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from polar) (3.4.8)\n",
      "Requirement already satisfied: python-pptx in /home/jupyter/.local/lib/python3.10/site-packages (from polar) (1.0.2)\n",
      "Requirement already satisfied: imblearn in /home/jupyter/.local/lib/python3.10/site-packages (from polar) (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn->polar) (0.10.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /home/jupyter/.local/lib/python3.10/site-packages (from python-pptx->polar) (3.2.5)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx->polar) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/jupyter/.local/lib/python3.10/site-packages (from python-pptx->polar) (4.14.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->polar) (0.5.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install polar seaborn pandas scikit-learn scipy matplotlib numpy nltk -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "k1ly5ox9pjasl9nafh8b2k",
    "execution": {
     "iopub.execute_input": "2025-07-03T19:38:32.603638Z",
     "iopub.status.busy": "2025-07-03T19:38:32.602531Z",
     "iopub.status.idle": "2025-07-03T19:38:35.378092Z",
     "shell.execute_reply": "2025-07-03T19:38:35.376453Z",
     "shell.execute_reply.started": "2025-07-03T19:38:32.603577Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6041/4186425657.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpolar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mRFSD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from-s3-folder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/polar/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpolar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/polar/polar.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marima_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholtwinters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleExpSmoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholtwinters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExponentialSmoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarimax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSARIMAX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/holtwinters/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from statsmodels.tsa.holtwinters.model import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mPY_SMOOTHERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSMOOTHERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mExponentialSmoothing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mHolt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/holtwinters/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m from statsmodels.tsa.exponential_smoothing.ets import (\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0m_initialization_heuristic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0m_initialization_simple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/exponential_smoothing/ets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    165\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtsbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexponential_smoothing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexponential_smoothing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ets_smooth\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m from statsmodels.tsa.exponential_smoothing.initialization import (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/exponential_smoothing/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpinv_extended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtsbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_cond\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/tools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_using_pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_best_blas_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from . import (_initialization, _representation, _kalman_filter,\n\u001b[0m\u001b[1;32m     15\u001b[0m                \u001b[0m_kalman_smoother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_simulation_smoother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                _cfa_simulation_smoother, _tools)\n",
      "\u001b[0;32mstatsmodels/tsa/statespace/_initialization.pyx\u001b[0m in \u001b[0;36minit statsmodels.tsa.statespace._initialization\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import polar\n",
    "RFSD = ds.dataset(\"from-s3-folder\", partitioning=\"hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T19:22:55.282604Z",
     "iopub.status.busy": "2025-07-03T19:22:55.281439Z",
     "iopub.status.idle": "2025-07-03T19:22:55.307850Z",
     "shell.execute_reply": "2025-07-03T19:22:55.306496Z",
     "shell.execute_reply.started": "2025-07-03T19:22:55.282558Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3286/3085518225.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m RFSD_2023_crit = pl.from_arrow(\n\u001b[0m\u001b[1;32m      2\u001b[0m     RFSD.to_table(\n\u001b[1;32m      3\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2023\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exemption_criteria'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "RFSD_2023_crit = pl.from_arrow(\n",
    "    RFSD.to_table(\n",
    "        filter=ds.field('year') == 2023,\n",
    "        columns=['inn', 'exemption_criteria']\n",
    "        )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "notebookId": "e5440af3-7591-406d-9891-a14b07f7558f",
  "notebookPath": "dataflow_en.ipynb",
  "ydsNotebookPath": "dataflow_ru.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
