{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "–ï—Å–ª–∏ —è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω–∏–º–∞—é, —Ç–æ –ø—Ä–∏ –º–Ω–æ–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç http –æ—à–∏–±–∫–∞ 509 –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ colab, –ø–æ—ç—Ç–æ–º—É –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ –Ω–∞–¥–æ —Å–∫–∞—á–∞—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å, —É–¥–∞–ª–∏—Ç—å —Å—Ä–µ–¥—É –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∑–∞–Ω–æ–≤–æ. –ü–æ—Ç–æ–º —Å–º–µ—Ä–∂–∏—Ç—å —Ñ–∞–π–ª—ã—ã –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"
      ],
      "metadata": {
        "id": "zR84qTVe_ang"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "\n",
        "HEADERS = {\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'accept-language': 'ru-RU,ru;q=0.9',\n",
        "    'cache-control': 'max-age=0',\n",
        "    # <--- COOKIE --->\n",
        "    'cookie': 'cookie',\n",
        "    'priority': 'u=0, i',\n",
        "    'referer': 'https://zoon.ru/msk/restaurants/', # –û–±—â–∏–π —Ä–µ—Ñ–µ—Ä–µ—Ä\n",
        "    'sec-ch-ua': '\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"Windows\"',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'cross-site',\n",
        "    'sec-fetch-user': '?1',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',\n",
        "}\n",
        "\n",
        "URLS = {\n",
        "    'msk': 'https://zoon.ru/msk/restaurants/type/kofejni/',\n",
        "    'spb': 'https://zoon.ru/spb/restaurants/type/kofejni/'\n",
        "}\n",
        "\n",
        "POSITIVE_KEYWORDS = ['–∫–æ—Ñ–µ–π–Ω—è', '–∫–æ—Ñ–µ —Å —Å–æ–±–æ–π', '–∫–∞—Ñ–µ', '–∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∞—è', '–ø–µ–∫–∞—Ä–Ω—è', '–∫–æ—Ñ–µ –Ω–∞ –≤—ã–Ω–æ—Å']\n",
        "BAN_WORDS = [\n",
        "    '—Å—Ç–µ–π–∫-—Ö–∞—É—Å', '—Ö–∏–Ω–∫–∞–ª—å–Ω–∞—è', '–∫–∞–ª—å—è–Ω–Ω–∞—è', '–ø–∞–±', '–ø–∏—Ü—Ü–µ—Ä–∏—è', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '—Ö–∏–Ω–∫–∞–ª—å–Ω–∞—è', '–≥—Ä–∏–ª—å-–±–∞—Ä', '—à–∞—à–ª—ã—á–Ω–∞—è'\n",
        "]\n",
        "\n",
        "\n",
        "def save_chunk_to_csv(data_list, city_code, start_page, end_page):\n",
        "    if not data_list:\n",
        "        return\n",
        "\n",
        "    filename = f\"{city_code}_coffee_{start_page - 1}_{end_page}.csv\"\n",
        "    df = pd.DataFrame(data_list)\n",
        "    df['city'] = city_code\n",
        "\n",
        "    cols_order = [\n",
        "        'city', 'name', 'zoon_id', 'rating', 'reviews_count', 'price_category', 'tags',\n",
        "        'address', 'metro_station', 'work_hours_short', 'phone', 'lat', 'lon',\n",
        "        'photo_count', 'url', 'photos_urls'\n",
        "    ]\n",
        "    existing_cols = [col for col in cols_order if col in df.columns]\n",
        "    df = df[existing_cols]\n",
        "\n",
        "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "    tqdm.write(f\"\\n–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(data_list)} –∑–∞–ø–∏—Å–µ–π –∑–∞ —Å—Ç—Ä. {start_page}-{end_page} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filename}\\n\")\n",
        "\n",
        "\n",
        "def get_establishment_tags(detail_url, session):\n",
        "    try:\n",
        "        time.sleep(random.uniform(0.5, 1.5))\n",
        "        response = session.get(detail_url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "        title_dt = soup.find('dt', attrs={'data-value': '–¢–∏–ø –∑–∞–≤–µ–¥–µ–Ω–∏—è'})\n",
        "        if not title_dt: return []\n",
        "        tags_dd = title_dt.find_next_sibling('dd')\n",
        "        if not tags_dd: return []\n",
        "        tags = [a.text.strip().lower() for a in tags_dd.find_all('a')]\n",
        "        return tags\n",
        "    except requests.RequestException as e:\n",
        "        tqdm.write(f\"  - –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Ç–µ–≥–æ–≤ {detail_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"  - –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–≥–∏ –Ω–∞ {detail_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def scrape_zoon_coffee(city_name, base_url):\n",
        "    page = 1\n",
        "    chunk_data = []\n",
        "    chunk_start_page = 1\n",
        "\n",
        "    MAX_PAGES = 50\n",
        "    CHUNK_SIZE = 5\n",
        "    MAX_RETRIES = 3\n",
        "    RETRY_DELAY = 45\n",
        "\n",
        "    print(f\" –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –≥–æ—Ä–æ–¥—É: {city_name.upper()}\")\n",
        "\n",
        "    with requests.Session() as session:\n",
        "        session.headers.update(HEADERS)\n",
        "\n",
        "        while page <= MAX_PAGES:\n",
        "            if page == 1:\n",
        "                url = base_url\n",
        "            else:\n",
        "                url = f\"{base_url}page-{page}/\"\n",
        "\n",
        "            page_processed_successfully = False\n",
        "\n",
        "            for attempt in range(MAX_RETRIES):\n",
        "                print(f\"\\n–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url} (–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{MAX_RETRIES})\")\n",
        "                try:\n",
        "                    response = session.get(url, timeout=20)\n",
        "                    response.raise_for_status()\n",
        "\n",
        "                    soup = BeautifulSoup(response.text, 'lxml')\n",
        "                    items = soup.find_all('li', class_='minicard-item')\n",
        "\n",
        "                    if not items and \"–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\" in response.text:\n",
        "                        page = MAX_PAGES + 1\n",
        "                        break\n",
        "\n",
        "                    for item in tqdm(items, desc=f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}\", unit=\" –∑–∞–≤–µ–¥–µ–Ω–∏–µ\"):\n",
        "                        data = {}\n",
        "                        title_tag = item.find('a', class_='title-link')\n",
        "                        data['name'] = title_tag.text.strip() if title_tag else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'\n",
        "                        data['url'] = title_tag['href'] if title_tag and title_tag.has_attr('href') else None\n",
        "                        if not data['url']:\n",
        "                            tqdm.write(f\"  -  –ü—Ä–æ–ø—É—Å–∫ '{data['name']}', —Ç.–∫. –Ω–µ—Ç URL\")\n",
        "                            continue\n",
        "\n",
        "                        tags = get_establishment_tags(data['url'], session)\n",
        "                        if tags is None:\n",
        "                            tqdm.write(f\"  - –ü—Ä–æ–ø—É—Å–∫ '{data['name']}' –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–≥–æ–≤.\")\n",
        "                            continue\n",
        "\n",
        "                        data['tags'] = ', '.join(sorted(list(set(tags))))\n",
        "                        has_positive = any(kw in tags for kw in POSITIVE_KEYWORDS)\n",
        "                        banned_found = [word for word in tags if word in BAN_WORDS]\n",
        "                        has_banned = bool(banned_found)\n",
        "\n",
        "                        if has_banned:\n",
        "                            reason = f\"–ù–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏: {banned_found}\"\n",
        "                            tqdm.write(f\"  - üö´ –ü—Ä–æ–ø—É—â–µ–Ω–æ: '{data['name']}' | –ü—Ä–∏—á–∏–Ω–∞: {reason} | –í—Å–µ —Ç–µ–≥–∏: {tags}\")\n",
        "                            continue\n",
        "                        elif not has_positive:\n",
        "                            reason = \"–ù–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –æ–¥–∏–Ω –∏–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤\"\n",
        "                            tqdm.write(f\"  - üö´ –ü—Ä–æ–ø—É—â–µ–Ω–æ: '{data['name']}' | –ü—Ä–∏—á–∏–Ω–∞: {reason} | –í—Å–µ —Ç–µ–≥–∏: {tags}\")\n",
        "                            continue\n",
        "                        else:\n",
        "                            tqdm.write(f\"  - ‚úÖ –ü—Ä–∏–Ω—è—Ç–æ: '{data['name']}' | –¢–µ–≥–∏: {tags}\")\n",
        "\n",
        "                        data['zoon_id'] = item.get('data-id')\n",
        "                        data['lat'] = item.get('data-lat')\n",
        "                        data['lon'] = item.get('data-lon')\n",
        "                        address_container = item.find('address', class_='minicard-item__address')\n",
        "                        if address_container:\n",
        "                            address_span = address_container.find('span', class_='address')\n",
        "                            data['address'] = address_span.text.strip() if address_span else None\n",
        "                            metro_tag = address_container.find('a', class_='metro')\n",
        "                            data['metro_station'] = metro_tag.text.strip() if metro_tag else None\n",
        "                        else:\n",
        "                            data['address'] = None\n",
        "                            data['metro_station'] = None\n",
        "                        rating_tag = item.select_one('div.stars div.z-text--bold')\n",
        "                        data['rating'] = rating_tag.text.strip().replace(',', '.') if rating_tag else None\n",
        "                        reviews_tag = item.select_one('div.comments')\n",
        "                        if reviews_tag:\n",
        "                            match = re.search(r'\\d+', reviews_tag.text)\n",
        "                            data['reviews_count'] = match.group(0) if match else None\n",
        "                        else:\n",
        "                            data['reviews_count'] = None\n",
        "                        work_time_tag = item.select_one('.minicard-item__work-time span:last-child')\n",
        "                        data['work_hours_short'] = work_time_tag.text.strip() if work_time_tag else None\n",
        "                        phone_tag = item.find('span', class_='js-phone')\n",
        "                        if phone_tag and phone_tag.has_attr('data-json'):\n",
        "                            try:\n",
        "                                phone_json = json.loads(phone_tag['data-json'])\n",
        "                                data['phone'] = phone_json.get('formatted')\n",
        "                            except json.JSONDecodeError:\n",
        "                                data['phone'] = None\n",
        "                        else:\n",
        "                            data['phone'] = None\n",
        "                        price_tag = item.select_one('.price-category')\n",
        "                        if price_tag:\n",
        "                            all_spans = price_tag.find_all('span')\n",
        "                            filled_count = len([s for s in all_spans if '_deselected' not in s.get('class', [])])\n",
        "                            data['price_category'] = f\"{filled_count}/{len(all_spans)}\"\n",
        "                        else:\n",
        "                            data['price_category'] = None\n",
        "                        photo_count_tag = item.select_one('.controls__item.count')\n",
        "                        if photo_count_tag:\n",
        "                            match = re.search(r'\\d+', photo_count_tag.text)\n",
        "                            data['photo_count'] = match.group(0) if match else None\n",
        "                        else:\n",
        "                            data['photo_count'] = None\n",
        "                        photos_tag = item.find('div', class_='js-slider-block')\n",
        "                        if photos_tag and photos_tag.has_attr('data-photos'):\n",
        "                            try:\n",
        "                                data['photos_urls'] = ', '.join(json.loads(photos_tag['data-photos']))\n",
        "                            except json.JSONDecodeError:\n",
        "                                data['photos_urls'] = None\n",
        "                        else:\n",
        "                            data['photos_urls'] = None\n",
        "\n",
        "                        chunk_data.append(data)\n",
        "\n",
        "                    page_processed_successfully = True\n",
        "                    break\n",
        "\n",
        "                except requests.RequestException as e:\n",
        "                    print(f\"  - –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}\")\n",
        "                    if attempt < MAX_RETRIES - 1:\n",
        "                        print(f\"  - –û–∂–∏–¥–∞–Ω–∏–µ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...\")\n",
        "                        time.sleep(RETRY_DELAY)\n",
        "                    else:\n",
        "                        print(f\"  - –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫. –ü—Ä–µ—Ä—ã–≤–∞—é —Ä–∞–±–æ—Ç—É –ø–æ –≥–æ—Ä–æ–¥—É.\")\n",
        "                        page = MAX_PAGES + 1\n",
        "                        break\n",
        "\n",
        "            if page_processed_successfully:\n",
        "                if page % CHUNK_SIZE == 0:\n",
        "                    save_chunk_to_csv(chunk_data, city_name, chunk_start_page, page)\n",
        "                    chunk_data = []\n",
        "                    chunk_start_page = page + 1\n",
        "                page += 1\n",
        "                time.sleep(random.uniform(3, 5))\n",
        "\n",
        "    if chunk_data:\n",
        "        last_scraped_page = page - 1\n",
        "        save_chunk_to_csv(chunk_data, city_name, chunk_start_page, last_scraped_page)\n",
        "\n",
        "    print(f\"\\n‚úÖ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –≥–æ—Ä–æ–¥—É {city_name.upper()} –∑–∞–≤–µ—Ä—à–µ–Ω.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –æ–±–Ω–æ–≤–∏—Ç–µ 'cookie' –≤ —Å–ª–æ–≤–∞—Ä–µ HEADERS!\n",
        "    for city_code, city_url in URLS.items():\n",
        "        scrape_zoon_coffee(city_code, city_url)\n",
        "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "iC3cgC4Y82sW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}